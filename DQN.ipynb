{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76ba9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from collections import deque, namedtuple, OrderedDict\n",
    "import random\n",
    "import time\n",
    "from typing import Iterator, List, NamedTuple, Optional, Tuple\n",
    "\n",
    "# Third party imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import ImageGrab\n",
    "from pynput.keyboard import Key, Controller\n",
    "from pytesseract import image_to_string\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam, Optimizer\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1734a35e",
   "metadata": {},
   "source": [
    "# Environment\n",
    "I tried to make this resemble OpenAI Gym's environment class `gym.Env`, just based on their [\"Getting Started\" page](https://gym.openai.com/docs/#environments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b87d82b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAME_WINDOW_BBOX    = (490,130,1485,575) # Tuned for my own display, crops out the floor\n",
    "GAME_OVER_TEXT_BBOX = (850,350,1125,400) # Tuned for my own display\n",
    "STAY_ALIVE_REWARD   = 0.1                # The longer the episode, the better\n",
    "JUMP_REWARD         = -1                 # Minor penalty, to discourage from constant jumping\n",
    "TARGET_IMAGE_SIZE   = (84,84)            # The width and height of the screen after re-sizing\n",
    "IMAGE_STACK_DEPTH   = 4                  # The number of images to stack for model input\n",
    "        \n",
    "\n",
    "class PenguinRunEnvironment:    \n",
    "    \"\"\"\n",
    "    Provides an interface for interacting with the Penguin Run game.\n",
    "    \n",
    "    ----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    game_window_bbox: A 4-tuple of the form (x1,y1,x2,y2), where (x1,y1) is the\n",
    "        coordinate of the upper-left bound and (x2,y2) is the coordinate of the\n",
    "        lower-left bound of the game window.\n",
    "    game_over_text_bbox: A 4-tuple of the form (x1,y1,x2,y2), where (x1,y1) is\n",
    "        the coordinate of the upper-left bound and (x2,y2) is the coordinate of\n",
    "        the lower-left bound of where the text \"GAME OVER\" appears in the game\n",
    "        window.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 game_window_bbox: Tuple[int,int,int,int],\n",
    "                 game_over_text_bbox: Tuple[int,int,int,int]) -> None:\n",
    "        self.game_window_bbox = game_window_bbox\n",
    "        self.game_over_text_bbox = game_over_text_bbox\n",
    "        self.keyboard = Controller()\n",
    "        self.action_space = [0, 1] # [NO JUMP, JUMP]\n",
    "        self.n_actions = len(self.action_space)\n",
    "        self.stack_depth = IMAGE_STACK_DEPTH\n",
    "        self.target_image_size = TARGET_IMAGE_SIZE\n",
    "        self.frame_stack = self._create_clean_image_deque()\n",
    "        \n",
    "    def reset(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Presses the space bar to restart the game. (Assumes the game window is \n",
    "        currently in focus and is in the \"GAME OVER\" state.\n",
    "        \n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        A screenshot of the game window.\n",
    "        \"\"\"\n",
    "        time.sleep(2) # Give a short delay so we can switch windows, if needed\n",
    "        self.frame_stack = self._create_clean_image_deque()\n",
    "        self.keyboard.press(Key.space)\n",
    "        self.keyboard.release(Key.space)\n",
    "        obs = self._observe(is_new_episode=True)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action: int) -> (np.ndarray, float, bool): \n",
    "        \"\"\"\n",
    "        Performs the given action in the environment.\n",
    "        \n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        action: The action to perform (0 or 1).\n",
    "        \n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        A 3-tuple where the first element is a screenshot of the\n",
    "        game, the second element is the reward achieved by the\n",
    "        given action, and the third element is a boolean indicating\n",
    "        whether the environment should be reset.\n",
    "        \"\"\"\n",
    "        assert action in self.action_space\n",
    "        reward = 0\n",
    "        \n",
    "        if action == 0:\n",
    "            self.keyboard.press(Key.space)\n",
    "            self.keyboard.release(Key.space)\n",
    "            reward += JUMP_REWARD\n",
    "        elif action == 1:\n",
    "            pass\n",
    "        \n",
    "        obs = self._observe()\n",
    "        needs_reset = self._is_game_over()\n",
    "        reward += STAY_ALIVE_REWARD if not needs_reset else 0\n",
    "        return (obs, reward, needs_reset)\n",
    "        \n",
    "    def sample_action(self) -> int:\n",
    "        \"\"\"        \n",
    "        Gets an action randomly chosen from the action space.\n",
    "        \"\"\"\n",
    "        return random.choice(self.action_space)\n",
    "\n",
    "    def _observe(self, is_new_episode: Optional[bool] = False) -> np.ndarray:\n",
    "        \"\"\"        \n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        is_new_episode: If the environment was just reset.\n",
    "\n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        A observation containing a screenshot and a deque of times\n",
    "        since previous jumps.\n",
    "        \"\"\"\n",
    "        pil_img = (ImageGrab.grab(bbox=self.game_window_bbox)\n",
    "                   .convert('L')\n",
    "                   .resize(self.target_image_size))\n",
    "        screen = np.array(pil_img) / 255.0\n",
    "        obs = self._stack_frames(screen, is_new_episode)\n",
    "        return obs\n",
    "    \n",
    "    def _is_game_over(self) -> bool:\n",
    "        \"\"\"     \n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        True, if the screen says \"GAME OVER\". False, otherwise.\n",
    "        \"\"\"\n",
    "        pil_img = ImageGrab.grab(bbox=self.game_over_text_bbox)\n",
    "        screen = np.array(pil_img)\n",
    "        try:\n",
    "            txt = image_to_string(screen, timeout=0.1).strip()\n",
    "            game_over = (txt == 'GAME OVER')\n",
    "        except:\n",
    "            game_over = False\n",
    "        return game_over\n",
    "    \n",
    "    def _create_clean_image_deque(self) -> deque:\n",
    "        \"\"\"      \n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        A clean image deque.\n",
    "        \"\"\"\n",
    "        return deque([np.zeros(self.target_image_size, dtype=np.float32) \\\n",
    "                      for i in range(self.stack_depth)], maxlen=4)\n",
    "    \n",
    "    def _stack_frames(self, frame: np.ndarray, is_new_episode: bool) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        frame: A NumPy array representing the frame to add to the stack.\n",
    "        is_new_episode: If the environment was just reset.\n",
    "        \n",
    "        ----------\n",
    "        Returns\n",
    "        ----------\n",
    "        The stacked frames.\n",
    "        \"\"\"\n",
    "        if is_new_episode:\n",
    "            for i in range(self.frame_stack.maxlen):\n",
    "                self.frame_stack.append(frame)\n",
    "        else:\n",
    "            self.frame_stack.append(frame)\n",
    "        stacked_frame = np.stack(self.frame_stack, axis=2).T\n",
    "        return stacked_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7f64c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "EPISODE 0\n",
      "Result: -75\n",
      "**********\n",
      "EPISODE 1\n",
      "Result: -126\n",
      "**********\n",
      "EPISODE 2\n",
      "Result: -8\n"
     ]
    }
   ],
   "source": [
    "# Test environment interaction\n",
    "env = PenguinRunEnvironment(GAME_WINDOW_BBOX, GAME_OVER_TEXT_BBOX)\n",
    "episodes = 3\n",
    "for i in range(episodes):\n",
    "    print('*' * 10)\n",
    "    print(f'EPISODE {i}')\n",
    "    env.reset()\n",
    "    time.sleep(1)\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    while not done:\n",
    "        action = env.sample_action()\n",
    "        obs, reward, done = env.step(action)\n",
    "        cumulative_reward += reward\n",
    "        time.sleep(0.1)\n",
    "    print(f'Result: {cumulative_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b45fc9",
   "metadata": {},
   "source": [
    "# Deep Q-Network\n",
    "Based on the [PyTorch Lightning tutorial](https://pytorch-lightning.readthedocs.io/en/latest/notebooks/lightning_examples/reinforce-learning-DQN.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d5be8bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            # ~> (4,84,84)\n",
    "            nn.Conv2d(in_channels=IMAGE_STACK_DEPTH,\n",
    "                      out_channels=32,\n",
    "                      kernel_size=(8,8),\n",
    "                      stride=(4,4)),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            # ~> (32,20,20)\n",
    "            nn.Conv2d(in_channels=32,\n",
    "                      out_channels=64,\n",
    "                      kernel_size=(4,4),\n",
    "                      stride=(2,2)),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            # ~> (64,9,9)\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=128,\n",
    "                      kernel_size=(4,4),\n",
    "                      stride=(2,2)),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            # ~> (3,3,128)\n",
    "            nn.Flatten(),\n",
    "            # ~> (1152)\n",
    "            nn.Linear(in_features=1152,\n",
    "                      out_features=512),\n",
    "            nn.ReLU(),\n",
    "            # ~> (512)\n",
    "            nn.Linear(in_features=512,\n",
    "                      out_features=2)\n",
    "            # ~> (2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.net(x)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "615af0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 84, 84])\n",
      "torch.float32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0754,  0.1045]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test DQN\n",
    "def create_img_stack():\n",
    "    img_stack = []\n",
    "    for i in range(4):\n",
    "        pil_img = ImageGrab.grab(bbox=GAME_WINDOW_BBOX).convert('L').resize(TARGET_IMAGE_SIZE)\n",
    "        np_arr = np.array(pil_img) / 255.0\n",
    "        img_stack.append(np_arr)\n",
    "        time.sleep(0.02)\n",
    "    img_stack = torch.Tensor(np.array(img_stack))\n",
    "    return img_stack\n",
    "\n",
    "x = create_img_stack()\n",
    "print(x.shape)\n",
    "print(x.dtype)\n",
    "dqn = DQN()\n",
    "dqn.forward(x.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6763d40",
   "metadata": {},
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "82e31636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named tuple for storing experience steps gathered in training\n",
    "Experience = namedtuple(\n",
    "    'Experience',\n",
    "    field_names=['state', 'action', 'reward', 'done', 'new_state'],\n",
    ")\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Replay Buffer for storing past experiences allowing the agent to learn from them.\n",
    "\n",
    "    ----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    capacity: Size of the buffer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, capacity: int) -> None:\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def __len__(self) -> None:\n",
    "        return len(self.buffer)\n",
    "\n",
    "    def append(self, experience: Experience) -> None:\n",
    "        \"\"\"\n",
    "        Adds the given experience to the buffer.\n",
    "        \n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        experience: A tuple of the form (state, action, reward, done, new_state).\n",
    "        \"\"\"\n",
    "        self.buffer.append(experience)\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple:\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        states, actions, rewards, dones, next_states = zip(*(self.buffer[idx] for idx in indices))\n",
    "\n",
    "        return (\n",
    "            np.array(states, dtype=np.float32),\n",
    "            np.array(actions),\n",
    "            np.array(rewards, dtype=np.float32),\n",
    "            np.array(dones, dtype=bool),\n",
    "            np.array(next_states, dtype=np.float32),\n",
    "        )\n",
    "    \n",
    "class RLDataset(IterableDataset):\n",
    "    \"\"\"\n",
    "    Iterable Dataset containing the ExperienceBuffer which will be updated with\n",
    "    new experiences during training.\n",
    "\n",
    "    ----------\n",
    "    Parameters\n",
    "    ----------\n",
    "    buffer: Replay buffer.\n",
    "    sample_size: Number of experiences to sample at a time.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, buffer: ReplayBuffer, sample_size: Optional[int] = 200) -> None:\n",
    "        self.buffer = buffer\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self) -> Iterator:\n",
    "        states, actions, rewards, dones, new_states = self.buffer.sample(self.sample_size)\n",
    "        for i in range(len(dones)):\n",
    "            yield states[i], actions[i], rewards[i], dones[i], new_states[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746939ee",
   "metadata": {},
   "source": [
    "# Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e31e84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "    Handles interaction with the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, env: PenguinRunEnvironment, replay_buffer: ReplayBuffer) -> None:\n",
    "        self.env = env\n",
    "        self.replay_buffer = replay_buffer\n",
    "        time.sleep(2) # Time to switch windows\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        self.state = self.env.reset()\n",
    "        \n",
    "    def get_action(self, net: nn.Module, epsilon: float, device: str) -> int:\n",
    "        if np.random.random() < epsilon:\n",
    "            action = self.env.sample_action()\n",
    "        else:\n",
    "            state = torch.FloatTensor([self.state])\n",
    "            \n",
    "            if device != 'cpu':\n",
    "                state = state.cuda(device)\n",
    "            \n",
    "            q_values = net(state)\n",
    "            _, action = torch.max(q_values, dim=1)\n",
    "            action = int(action.item())\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def play_step(\n",
    "        self,\n",
    "        net: nn.Module,\n",
    "        epsilon: float = 0.0,\n",
    "        device: str = 'cpu'\n",
    "    ) -> Tuple[float, bool]:\n",
    "        action = self.get_action(net, epsilon, device)\n",
    "        new_state, reward, done = self.env.step(action)\n",
    "        exp = Experience(self.state, action, reward, done, new_state)\n",
    "        self.replay_buffer.append(exp)\n",
    "        self.state = new_state\n",
    "        if done:\n",
    "            self.reset()\n",
    "        return reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f43ce563",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Test Agent\n",
    "agent = Agent(PenguinRunEnvironment(GAME_WINDOW_BBOX, GAME_OVER_TEXT_BBOX), ReplayBuffer(1000))\n",
    "action = agent.get_action(DQN(), epsilon=0, device='cpu')\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084fb036",
   "metadata": {},
   "source": [
    "# DQN Lightning Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "2e844b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLightning(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        batch_size: int = 16,\n",
    "        lr: float = 1e-4,\n",
    "        gamma: float = 0.99,\n",
    "        sync_rate: int = 10,\n",
    "        replay_size: int = 1000,\n",
    "        warm_start_size: int = 1000,\n",
    "        eps_last_frame: int = 1000,\n",
    "        eps_start: float = 1.0,\n",
    "        eps_end: float = 0.01,\n",
    "        episode_length: int = 200,\n",
    "        warm_start_steps: int = 1000\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: Size of the batches.\n",
    "        lr: Learning rate.\n",
    "        gamma: Discount factor.\n",
    "        sync_rate: How many frames do we update the target network.\n",
    "        replay_size: Capacity of the replay buffer.\n",
    "        warm_start_size: How many samples do we use to fill our buffer\n",
    "            at the start of training.\n",
    "        eps_last_frame: What frame should epsilon stop decaying.\n",
    "        eps_start: Epsilon starting value.\n",
    "        eps_end: Epsilon final value.\n",
    "        episode_length: Max length of an episode.\n",
    "        warm_start_steps: Max episode reward in the environment.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.env = PenguinRunEnvironment(GAME_WINDOW_BBOX, GAME_OVER_TEXT_BBOX)\n",
    "\n",
    "        self.net = DQN()\n",
    "        self.target_net = DQN()\n",
    "\n",
    "        self.buffer = ReplayBuffer(self.hparams.replay_size)\n",
    "        self.agent = Agent(self.env, self.buffer)\n",
    "        self.total_reward = 0\n",
    "        self.episode_reward = 0\n",
    "        self.populate(self.hparams.warm_start_steps)\n",
    "    \n",
    "    def populate(self, steps: int = 1000) -> None:\n",
    "        \"\"\"\n",
    "        Carries out several random steps through the environment to initially fill\n",
    "        up the replay buffer with experiences\n",
    "\n",
    "        ----------\n",
    "        Parameters\n",
    "        ----------\n",
    "        steps: Number of random steps to populate the buffer with.\n",
    "        \"\"\"\n",
    "        time.sleep(2) # Gives time to change windows\n",
    "        for i in range(steps):\n",
    "            self.agent.play_step(self.net, epsilon=1.0)\n",
    "            \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        output = self.net(x)\n",
    "        return output\n",
    "    \n",
    "    def dqn_mse_loss(self, batch: Tuple[torch.Tensor, torch.Tensor]) -> torch.Tensor:\n",
    "        states, actions, rewards, dones, next_states = batch\n",
    "        state_action_values = self.net(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_state_values = self.target_net(next_states).max(1)[0]\n",
    "            next_state_values[dones] = 0.0\n",
    "            next_state_values = next_state_values.detach()\n",
    "        \n",
    "        expected_state_action_values = next_state_values * self.hparams.gamma + rewards\n",
    "        \n",
    "        return nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "    \n",
    "    def training_step(self, batch: Tuple[torch.Tensor, torch.Tensor], nb_batch) -> OrderedDict:\n",
    "        device = self.get_device(batch)\n",
    "        epsilon = max(\n",
    "            self.hparams.eps_end,\n",
    "            self.hparams.eps_start - self.global_step + 1 / self.hparams.eps_last_frame,\n",
    "        )\n",
    "\n",
    "        # step through environment with agent\n",
    "        reward, done = self.agent.play_step(self.net, epsilon, device)\n",
    "        self.episode_reward += reward\n",
    "\n",
    "        # calculates training loss\n",
    "        loss = self.dqn_mse_loss(batch)\n",
    "\n",
    "        if done:\n",
    "            self.total_reward = self.episode_reward\n",
    "            self.episode_reward = 0\n",
    "\n",
    "        # Soft update of target network\n",
    "        if self.global_step % self.hparams.sync_rate == 0:\n",
    "            self.target_net.load_state_dict(self.net.state_dict())\n",
    "\n",
    "        log = {\n",
    "            'total_reward': torch.tensor(self.total_reward).to(device),\n",
    "            'reward': torch.tensor(reward).to(device),\n",
    "            'train_loss': loss\n",
    "        }\n",
    "        status = {\n",
    "            'steps': torch.tensor(self.global_step).to(device),\n",
    "            'total_reward': torch.tensor(self.total_reward).to(device)\n",
    "        }\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self) -> List[Optimizer]:\n",
    "        \"\"\" Initialize Adam optimizer\"\"\"\n",
    "        optimizer = Adam(self.net.parameters(), lr=self.hparams.lr)\n",
    "        return [optimizer]\n",
    "\n",
    "    def __dataloader(self) -> DataLoader:\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences\"\"\"\n",
    "        dataset = RLDataset(self.buffer, self.hparams.episode_length)\n",
    "        dataloader = DataLoader(\n",
    "            dataset=dataset,\n",
    "            batch_size=self.hparams.batch_size,\n",
    "        )\n",
    "        return dataloader\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader:\n",
    "        \"\"\"Get train loader\"\"\"\n",
    "        return self.__dataloader()\n",
    "\n",
    "    def get_device(self, batch) -> str:\n",
    "        \"\"\"Retrieve device currently being used by minibatch\"\"\"\n",
    "        return batch[0].device.index if self.on_gpu else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c70ad",
   "metadata": {},
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6aab7bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type | Params\n",
      "------------------------------------\n",
      "0 | net        | DQN  | 764 K \n",
      "1 | target_net | DQN  | 764 K \n",
      "------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "6.113     Total estimated model params size (MB)\n",
      "/home/school/anaconda3/envs/deep-rl/lib/python3.9/site-packages/pytorch_lightning/trainer/data_loading.py:105: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619614622a5347049aacfd54e5355a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: -1it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/school/anaconda3/envs/deep-rl/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:1046: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "model = DQNLightning()\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "trainer = pl.Trainer(\n",
    "    gpus=AVAIL_GPUS,\n",
    "    max_epochs=200,\n",
    "    val_check_interval=100,\n",
    ")\n",
    "\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "9aabb2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "states, actions, rewards, dones, next_states = model.agent.replay_buffer.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a15b8eed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f3c2a4d26d0>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(np.array(states[0,3,:,:].T), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a74e0b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1], dtype=float32)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c2d34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
